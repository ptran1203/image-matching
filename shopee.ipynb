{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "shopee.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ptran1203/image-matching/blob/master/shopee.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0uHdpVz7Hoh"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "print(torch.cuda.get_device_properties(0))\n",
        "assert torch.cuda.get_device_properties(0).total_memory > 12996954624, \"Too weak GPU\"\n",
        "from google.colab import output, drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "data_name = \"shopee-product-matching\"\n",
        "if not os.path.exists(\"/content/train_images\"):\n",
        "    !pip3 uninstall -y kaggle\n",
        "    !pip3 install kaggle==1.5.12\n",
        "    !pip install --upgrade seaborn\n",
        "    !echo \"{\\\"username\\\":\\\"ptran1203\\\",\\\"key\\\":\\\"<your key>\\\"}\" > kaggle.json\n",
        "    if not os.path.exists(\"~/.kaggle\"):\n",
        "        !mkdir ~/.kaggle\n",
        "    !cp kaggle.json ~/.kaggle/\n",
        "    !chmod 600 ~/.kaggle/kaggle.json\n",
        "    !kaggle competitions download -c shopee-product-matching\n",
        "    !unzip {data_name}.zip -d /content\n",
        "    # !rm {data_name}.zip\n",
        "\n",
        "    if not os.path.exists(\"/content/train_images\"):\n",
        "        raise ValueError(f\"Download Failed, {data_name}\")\n",
        "\n",
        "output.clear()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sI4FOs77A1uL"
      },
      "source": [
        "%cd /content\n",
        "!rm -rf image-matching\n",
        "!git clone https://github.com/ptran1203/image-matching\n",
        "%cd image-matching\n",
        "!pip3 install -r requirements.txt\n",
        "output.clear()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAsIqAPfJUMT"
      },
      "source": [
        "# Rename train.csv and add fold\n",
        "!mv /content/train.csv /content/train_.csv\n",
        "\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import models\n",
        "import dataset\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def to_str(row):\n",
        "    return ' '.join(row)\n",
        "\n",
        "df = pd.read_csv('/content/train_.csv')\n",
        "df['label_group'] =  LabelEncoder().fit_transform(df.label_group)\n",
        "tmp = df.groupby('label_group').posting_id.agg('unique').to_dict()\n",
        "df['target'] = df.label_group.map(tmp)\n",
        "df['target'] = df['target'].apply(to_str)\n",
        "skf = GroupKFold(5)\n",
        "x = np.zeros(len(df))\n",
        "y = df.label_group.values\n",
        "\n",
        "df['fold'] = -1\n",
        "for i, (train_idx, valid_idx) in enumerate(skf.split(df, None, df['label_group'])):\n",
        "    df.loc[valid_idx, 'fold'] = i\n",
        "\n",
        "df.to_csv('/content/train.csv', index=False)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXNSRC-65W0J"
      },
      "source": [
        "transforms_train, transforms_val = dataset.get_transforms(336, stage=1, norm=False)\n",
        "train_df = dataset.get_df()[0].sample(frac=1)\n",
        "train_data = dataset.ShoppeDataset(train_df, transform=transforms_train)\n",
        "\n",
        "for img, label in train_data:\n",
        "    plt.imshow(img.squeeze(0).permute(1, 2, 0))\n",
        "    break\n",
        "label\n",
        "# raise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t319LKlsBXeX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb07b13e-4fbe-44cc-ce02-2f13fa5ae89c"
      },
      "source": [
        "def get_weight(stage):\n",
        "    if stage == 1:\n",
        "        stage = 2\n",
        "\n",
        "    load_from = os.path.join(PROJECT_DIR, 'weights', f'{model_type}_fold{fold}_stage{stage-1}.pth')\n",
        "    if not os.path.exists(load_from):\n",
        "        print(f'{load_from} not exist')\n",
        "        load_from = 'none'\n",
        "    return load_from\n",
        "\n",
        "# nest101 rex20 tf_efficientnet_b5_ns\n",
        "PROJECT_DIR = '/content/drive/MyDrive/shopee'\n",
        "\n",
        "model_type = 'tf_efficientnet_b1_ns'\n",
        "fold = 0\n",
        "stage = 1\n",
        "\n",
        "image_size, batch_size, n_epochs, init_lr = {\n",
        "    1: [224, 16, 6, 0.0001],\n",
        "    2: [448, 8, 6, 0.00001],\n",
        "    3: [640, 4, 4, 0.00001]\n",
        "}[stage]\n",
        "\n",
        "load_from = get_weight(stage)\n",
        "# load_from='none'\n",
        "print(load_from)\n",
        "\n",
        "!python3 train.py --kernel-type {model_type}\\\n",
        "                --num-workers 2\\\n",
        "                --image-size {image_size}\\\n",
        "                --enet-type {model_type}\\\n",
        "                --model-dir {PROJECT_DIR}/weights\\\n",
        "                --log-dir {PROJECT_DIR}/logs\\\n",
        "                --batch-size {batch_size}\\\n",
        "                --fold {fold}\\\n",
        "                --load-from {load_from}\\\n",
        "                --n-epochs {n_epochs}\\\n",
        "                --init-lr {init_lr}\\\n",
        "                --stage {stage}\\\n",
        "\n",
        "raise"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "none\n",
            "Namespace(DEBUG=False, batch_size=16, data_dir='/raid/GLD2', enet_type='tf_efficientnet_b1_ns', fold=0, groups=0, image_size=224, init_lr=0.0001, kernel_type='tf_efficientnet_b1_ns', load_from='none', log_dir='/content/drive/MyDrive/shopee/logs', loss_config='{\"loss_type\": \"aarc\", \"margin\": 0.3, \"scale\": 30, \"label_smoothing\": 0.0}', model_dir='/content/drive/MyDrive/shopee/weights', n_epochs=6, num_workers=2, stage=1, start_from_epoch=1, stop_at_epoch=999, warmup_epochs=1)\n",
            "['posting_id', 'image', 'image_phash', 'title', 'label_group', 'target', 'fold']\n",
            "         posting_id  ... fold\n",
            "0   train_129225211  ...    3\n",
            "1  train_3386243561  ...    3\n",
            "2  train_2288590299  ...    4\n",
            "3  train_2406599165  ...    3\n",
            "4  train_3369186413  ...    1\n",
            "\n",
            "[5 rows x 7 columns]\n",
            "out_dim = 11014\n",
            "Adaptive margin [0.42183875 0.5        0.20724849 ... 0.5        0.5        0.37099592]\n",
            "Train on 27399 images, validate on 6851 images\n",
            "Tue Apr  6 15:19:41 2021 Epoch: 1\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "  0% 0/1712 [00:00<?, ?it/s]/content/image-matching/losses.py:202: UserWarning: This overload of addmm_ is deprecated:\n",
            "\taddmm_(Number beta, Number alpha, Tensor mat1, Tensor mat2)\n",
            "Consider using one of the following signatures instead:\n",
            "\taddmm_(Tensor mat1, Tensor mat2, *, Number beta, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)\n",
            "  dist.addmm_(1, -2, x, y.t())\n",
            "loss: 20.18441, smth: 19.68744, acc: 12.50000:  50% 858/1712 [03:55<03:53,  3.66it/s]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdwCNbGuK0Iw"
      },
      "source": [
        "from models import EffnetV2, RexNet20, ResNest101\n",
        "\n",
        "model = EffnetV2(model_type, out_dim=11014, pretrained=False)\n",
        "model = model.cuda()\n",
        "weight = get_weight(stage + 1)\n",
        "print(weight)\n",
        "checkpoint = torch.load(weight, map_location='cuda:0')\n",
        "state_dict = checkpoint['model_state_dict']\n",
        "state_dict = {k[7:] if k.startswith('module.') else k: state_dict[k] for k in state_dict.keys()}\n",
        "model.load_state_dict(state_dict, strict=True)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYcLAcfbOCVV"
      },
      "source": [
        "from tqdm import tqdm_notebook\n",
        "from torch.utils.data import DataLoader\n",
        "from dataset import ShoppeDataset\n",
        "import gc\n",
        "import cupy as cp\n",
        "import numpy as np\n",
        "\n",
        "def inference(model, test_loader):\n",
        "    embeds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for img in tqdm_notebook(test_loader): \n",
        "            img = img.cuda()\n",
        "            feat, _ = model(img)\n",
        "\n",
        "            image_embeddings = feat.detach().cpu().numpy()\n",
        "            embeds.append(image_embeddings)\n",
        "        \n",
        "    _ = gc.collect()\n",
        "    image_embeddings = np.concatenate(embeds)\n",
        "    return image_embeddings\n",
        "\n",
        "transforms_train, transforms_val = dataset.get_transforms(image_size)\n",
        "df_test = df[df['fold'] == fold].reset_index()\n",
        "dataset_test = ShoppeDataset(df_test, 'test', transform=transforms_val)\n",
        "test_loader = DataLoader(dataset_test, batch_size=16, num_workers=4)\n",
        "embeds = inference(model, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNFfCdABYxuT"
      },
      "source": [
        "embeds = cp.array(embeds)\n",
        "\n",
        "preds = []\n",
        "CHUNK = 1024 * 4\n",
        "\n",
        "print('Finding similar images...')\n",
        "CTS = len(embeds) // CHUNK\n",
        "\n",
        "if len(embeds) % CHUNK!=0:\n",
        "    CTS += 1\n",
        "\n",
        "for j in range(CTS):\n",
        "\n",
        "    a = j * CHUNK\n",
        "    b = (j + 1) * CHUNK\n",
        "    b = min(b,len(embeds))\n",
        "    print('chunk',a,'to',b)\n",
        "   \n",
        "    cts = cp.matmul(embeds, embeds[a:b].T).T\n",
        "    \n",
        "    for k in range(b-a):\n",
        "        # print(sorted(cts[k,], reverse=True))\n",
        "        IDX = cp.where(cts[k,]>=0.5)[0]\n",
        "        o = df_test.iloc[cp.asnumpy(IDX)].posting_id.values\n",
        "        preds.append(o)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4K6UuGceFCYw"
      },
      "source": [
        "from PIL import Image\n",
        "\n",
        "def get_path_by_id(id, df):\n",
        "    fname = df.loc[df.posting_id == id].image.values[0]\n",
        "    return f'/content/train_images/{fname}'\n",
        "def show_img_array(imgs):\n",
        "    max_imgs = 5\n",
        "    if len(imgs) > max_imgs:\n",
        "        imgs = imgs[:max_imgs]\n",
        "        print(f'WARNING: total images is {len(imgs)}, but only show at most {max_imgs} images')\n",
        "    \n",
        "    fig = plt.figure(figsize=(15, 5))\n",
        "    \n",
        "    n = min(max_imgs, len(imgs))\n",
        "    for i in range(n):\n",
        "        fig.add_subplot(1, n, i + 1)\n",
        "        # fig.suptitle(titles[i])\n",
        "        plt.imshow(imgs[i])\n",
        "\n",
        "    plt.show(block=True)\n",
        "\n",
        "imgs = np.array([\n",
        "    np.array(Image.open(get_path_by_id(f, df_test)))\n",
        "    for f in preds[np.random.randint(100)][:5]\n",
        "])\n",
        "\n",
        "show_img_array(imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-N36RUtXRbnT"
      },
      "source": [
        "from util import row_wise_f1_score\n",
        "import numpy as np\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "scores, score = row_wise_f1_score(df_test.target, preds)\n",
        "score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRzgp5HJRdFu"
      },
      "source": [
        "## Upload weight"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1AhD3YRRcOK"
      },
      "source": [
        "name = \"shopee\"\n",
        "path = f'/content/{name}'\n",
        "metadata = {\n",
        "  \"title\": name,\n",
        "  \"id\": f\"ptran1203/{name}\",\n",
        "  \"licenses\": [\n",
        "    {\n",
        "      \"name\": \"CC0-1.0\"\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "!rm -rf {path}\n",
        "!mkdir {path}\n",
        "!mkdir /content/data\n",
        "!du -sh {PROJECT_DIR}/weights\n",
        "import json\n",
        "with open(f\"/content/data/dataset-metadata.json\", \"w\") as f:\n",
        "    json.dump(metadata,f, indent=2)\n",
        "\n",
        "!cp -R /content/image-matching {path}/\n",
        "!cp -R {PROJECT_DIR}/weights {path}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1-NeajKT8fs"
      },
      "source": [
        "!zip -r /content/data/data.zip {path}\n",
        "!kaggle datasets version -p /content/data -m \"update\"\n",
        "# !kaggle datasets create -p /content/data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6REIjRQVBeI"
      },
      "source": [
        "!rm -rf /content/data"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}